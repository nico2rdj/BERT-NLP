# BERT sur SQuAD 2.0

<p align="center" >

  ![bert](https://user-images.githubusercontent.com/22484369/73925375-3f426480-48ce-11ea-8f5d-bf92ab1ebe87.png)


</p>

<h3 align="center">
  üìÑ Diff√©rents Fine-Tuning de BERT sur le dataset SQuAD 2.0
</h3>
<p align="center">
  Projet NLP <br/>
  <small></small>
</p>

## Manuel d'utilisation

- Se placer √† la fin du jupyter notebook dans la cellule ci-dessous et mettre le paragraphe dans la variable doc et la question dans la variable q : 

![example](https://user-images.githubusercontent.com/22484369/73929430-11145300-48d5-11ea-8e14-a29becca0ff4.png)
 
    
   
    
    
    

## D√©tails de l'impl√©mentation de BERT


- Pour le preprocessing, nous avons utilis√© la libraire **transformers** avec la fonction **SquadV2Processor** pour r√©cup√©rer les questions et r√©ponses puis **squad_convert_examples_to_features** pour convertir les celles-ci en features que l'on donne en entr√©e de BERT. <br/>

Le format des entr√©es est le suivant : <br/>
![input](https://user-images.githubusercontent.com/22484369/73931543-c8f72f80-48d8-11ea-96e1-76f477e344f0.png)

 - Ces entr√©es repr√©sente : 
    - attention mask
    - entr√©e contenant la question et la r√©ponse sous forme id de token
    - position du d√©but de la r√©ponse
    - position de la fin de la r√©ponse

- Pour BERT, nous avons utilis√© l'architecture Fine-Tuned **BertForQuestionAnswering** de base avec des poids pr√©entra√Æn√©s 


![bert-ft](https://user-images.githubusercontent.com/22484369/73930902-a3b5f180-48d7-11ea-8145-b8a79f439229.png)

- Nous avons essay√© deux mod√®les diff√©rents en Fine Tuned la derni√®re couche:
  - L'un avec une couche Dense classique
  - L'autre avec une couche LSTM


## R√©sultats sur le dataset SQuAD 2.0



## License

- [MIT](LICENSE)

## Contributors
- Fares Embouazza
- Nicolas Martin
